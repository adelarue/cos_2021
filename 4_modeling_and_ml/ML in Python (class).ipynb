{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Overview\n",
    "\n",
    "Now that you've had some experience using R for data wrangling and presentation, we're going to move over to Python to look at **model training** and **model evaluation** in this session. These are the key components of the machine learning pipeline that inform our **model selection,** i.e. choosing an algorithm to solve our desired prediction task with data.\n",
    "\n",
    "We're going to get some practice for two common types of machine learning tasks:\n",
    "\n",
    "1. Regression (supervised learning)\n",
    "2. Classification (supervised learning)\n",
    "\n",
    "It's important to note that model training and evaluation are not followed sequentially in practice, but instead iterated over to obtain the final model. We'll try to get a sense of what this looks like in the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Python?\n",
    "The content in today's session ccould be accomplished in either R or Python. Python is more widely used for machine learning tasks, especially when considering neural networks (which will be discucssed on Thursday). We are introducing today's material in Python to give a flavor of a different programming language, including its syntax and functionality. The material for today's session is adapted from **COS 2020's Modeling and ML** session using R. Feel free to refer to last year's [slides](https://github.com/adelarue/cos_2020/tree/master/4_modeling_and_ml) for similar session content in R syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare our environment for today's session, we will begin by loading pandas, a core Python library for data analysis. Most of our machine learning work will be done using scikit-learn; we will load these libraries as we go through the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load the data\n",
    "listings_raw = pd.read_csv('../data/listings_with_amenities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the first 10 rows\n",
    "# listings_raw.head(10)\n",
    "\n",
    "## Look at the columns\n",
    "# listings_raw.columns;\n",
    "\n",
    "## Describe the data (similar to summary() in R)\n",
    "# listings_raw.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, let's do some basic data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_price(p):\n",
    "    p = p.replace('$','').replace(',','') # replace all '$' and ',', similar to in R\n",
    "    return float(p)\n",
    "\n",
    "# We will \"apply\" this function to eacch element of our price column, and replace the price column with these values\n",
    "listings_raw.loc[:,'price'] = listings_raw.loc[:,'price'].apply(lambda x: clean_price(x))\n",
    "listings_raw.loc[:,'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also clean up some outliers\n",
    "listings_clean = listings_raw.query('accommodates <= 10 & price <= 1000')\n",
    "listings_clean = listings_clean.query('maximum_nights <= 2000')\n",
    "\n",
    "# Filter by property type\n",
    "listings_clean = listings_clean[listings_clean['property_type'].isin([\"Apartment\", \"House\", \"Bed & Breakfast\", \"Condominium\", \"Loft\", \"Townhouse\"])]\n",
    "listings_clean = listings_clean[~listings_clean['neighbourhood_cleansed'].isin([\"Leather District\", \"Longwood Medical Area\"])]\n",
    "\n",
    "# Find columns with sufficient fill (at most 25% missing values)\n",
    "cols_keep = listings_clean.columns[listings_clean.isnull().mean() <= 0.25]\n",
    "listings_clean = listings_clean.loc[:,cols_keep]\n",
    "\n",
    "# Let's one-hot encode our property types\n",
    "listings_clean = pd.get_dummies(listings_clean, columns = ['property_type'], drop_first = True)\n",
    "\n",
    "# We're also going to filter to numeric columns for now- \n",
    "# You can use one-hot encoding to handle categorical variables, but today we'll stick to this.\n",
    "listings_clean = listings_clean.select_dtypes(include=np.number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are there missing values?\n",
    "missing_vals = listings_clean.isna().sum()\n",
    "missing_vals.index[missing_vals>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute the missing data - we will simply impute the mean. There are other options available in `sklearn.impute`.\n",
    "Notice that the missingness is in numeric columns only, which makes it easier to deal with. If there are categoric columns with missing data, different approaches need to be taken (e.g. mode imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(listings_clean)\n",
    "\n",
    "# Get the imputed values and convert back into a dataframe (it will return a matrix)\n",
    "listings = pd.DataFrame(imp_mean.transform(listings_clean), \n",
    "                        columns = listings_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our feature space looks like now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regression\n",
    "\n",
    "We're going to start by solving a regression problem.\n",
    "\n",
    "**Inputs**\n",
    "\n",
    "Regression is a supervised learning problem, which means that we have access to a dataset $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i = 1}^{n}$ where each $x_i \\in \\mathbb{R}^d$ and each $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "Note that for regression, the output, $y$, is *continuous*. If the output is discrete, such as whether or not an apartment listing will be rented (yes/no) or what town the unit is listed in (multiple classes: Boston vs. Cambridge vs. Somerville), we would use a *classification* algorithm. We will consider these in part 2 of the session.\n",
    " \n",
    "**Aim**\n",
    "\n",
    "The aim of a regression task is to find a function $h: \\mathbb{R}^d \\to \\mathbb{R}$ which allows us to compute an accurate prediction of the output, $y$, for any given input, $x$. These new $x$ and $y$ are *unseen*: that is, we assume they are identically distributed relative to the points in $\\mathcal{D}$, but independent.\n",
    "\n",
    "Exactly what we mean by *accuracy* will be discussed when we talk about model evaluation.\n",
    "\n",
    "The way we approach the problem of finding $h$ is by selecting a parameterised class of models, and solving an optimization problem to find the best model within this class according to a *loss function*. We usually repeat this process for several model classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Regression Task\n",
    "\n",
    "We're going to use the Boston Airbnb dataset, where each entry is a property listing, and aim to use the input variables available to us in order to predict the price of a listing.\n",
    "\n",
    "Note here that we haven't specified exactly which variables in the dataset will be used to define each $x_i$. This choice of **feature space** is part of the modeling process, and we will experiment with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "The first class of models we're going to try is linear models. This means we hypothesise that the output, $y$, can be described using a linear combination of the inputs, $x$: $h(x) = \\beta^{\\intercal} x$.\n",
    "\n",
    "**Question**: What are the tunable parameters (hyperparameters) for the class of OLS models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting an OLS Model \n",
    "\n",
    "Now, we need to choose exactly which inputs will be used for fitting the model. Which inputs might be predictive of price? Recall the column names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very simple model, let's just choose `accommodates` as the only input. To fit this model we will use linear regression in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = listings[['accommodates']]\n",
    "y = listings['price']\n",
    "\n",
    "# Select the model type\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to our data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've trained our first model! Let's look at the model output. You can see the elements of the `model` object by typing `model.` and pressing \"tab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 'coefficients' section. In the 'estimate' column, we see that the point estimates for the model coefficients say that the price estimate is \\\\$53.65 + \\\\$38.18*(# of people accommodated). \n",
    "\n",
    "To visualise the model, let's plot the fitted line. We'll use `matplotlib` for our plotting, which offers similar functionality to `ggplot2` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Start with a scatterplot of the data\n",
    "plt.plot(X, y, 'o')\n",
    "\n",
    "# Add in the regression line\n",
    "y_pred = model.predict(X)\n",
    "plt.plot(X, y_pred)\n",
    "plt.title('OLS Model Fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look the model residuals for each accommodation size to see how appropriate the linear model is for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # another helpful plotting library\n",
    "\n",
    "# We'll add the OLS predictions and residuals to our dataframe, and then plot them\n",
    "resid = (y - y_pred)\n",
    "listings['ols_pred'] = y_pred\n",
    "listings['ols_resid'] = resid\n",
    "sns.boxplot(x='accommodates', y='ols_resid', data=listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: What do these box plots tell us about the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation\n",
    "\n",
    "Now we're going to look at two measures of accuracy for regression models -- in other words, how well a model explains a dataset.\n",
    "\n",
    "The first is the mean squared error (MSE), which is simply computed as the mean of the squared residuals. Recall that the residuals are stored in the `resid` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(resid**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the RMSE, (root) MSE, from a model applied to a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(resid**2))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has built-in evaluation functions that can do this for you. We'll also print the results out using Python's number formatting to show the MSE/RMSE rounded to two decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = metrics.mean_squared_error(y, y_pred, squared = False)\n",
    "print(\"MSE = %.2f\" % mse)\n",
    "print(\"RMSE = %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this measure is highly affected by the scale of the data. We can also use the 'R squared' coefficient, which is more interepretable since it is on a standardized scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred, y_mean):\n",
    "    ss_res = ((y_true - y_pred)**2).sum()\n",
    "    ss_tot = ((y_true - y_mean)**2).sum()\n",
    "    return (1 - (ss_res/ss_tot))\n",
    "\n",
    "r2 = r_squared(y, y_pred, np.mean(y))\n",
    "print(\"R^2 = %.2f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The R squared value is what we will use to evaluate the performance of our models in this session. But, it's important to note that this is definitely not the only choice we could have made! Check out scikit-learn's full library of [evaluation metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) to see what else is out there.\n",
    "\n",
    "**Question**: What is the relationship between the loss function and the measure of accuracy used to evaluate a model? How do we know which is the best choice for each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, Validation and Testing Splits\n",
    "\n",
    "Recall that our ultimate goal in this supervised learning task is to be able to predict price ($y$) from an *unseen* set of inputs ($x$, although we are still playing around with the input variables which define it).\n",
    "\n",
    "When building the OLS model, we used the entire dataset. Simply taking the R squared value on this dataset as a measure of performance is clearly not fair -- since we want the model to generalise to unseen data.\n",
    "\n",
    "To address this problem, we often split the dataset into three different chunks:\n",
    "\n",
    "1. **Training data**: the data we build our models on.\n",
    "2. **Validation data**: the data we tune hyperparameters on.\n",
    "3. **Testing data**: the data we use to obtain a final estimate of performance.\n",
    "\n",
    "The validation set is like an 'intermediate' estimate of performance. If we didn't use the validation set for this purpose, the only way of selecting the best model from a model class would be to look at its performance on the training set or the testing set.\n",
    "\n",
    "`train_test_split` in scikit-learn provides an easy way of creating these data partitions. Let's start by just creating training/testing sets. We'll get to parameter tuning in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features =  listings.columns.drop(['id', 'scrape_id', 'host_id', 'latitude', 'longitude', \n",
    "                                   'availability_30','availability_60', 'availability_90', 'availability_365',\n",
    "                                   'host_total_listings_count','calculated_host_listings_count',\n",
    "                                   'price', 'ols_pred', 'ols_resid'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(listings[features], listings['price'], \n",
    "                                                    train_size = 0.75, random_state = 1)\n",
    "\n",
    "print(\"Training Size = %d\" % X_train.shape[0]) # To print integers, we can pass '%d' into our print statement \n",
    "print(\"Testing Size = %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Iteration\n",
    "\n",
    "Now that we're equipped to build models and evaluate their performance, let's start iterating to find better models.\n",
    "\n",
    "We've glossed over the precise choice of variables to use in order to explain price, so let's try a few different combinations. \n",
    "\n",
    "Now we'll write a function that accepts data (X,y) and splits into training/testing data, fits a model, and returns the R squared value on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ols(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse_train = metrics.mean_squared_error(y_train, y_train_pred, squared = False)\n",
    "    rmse_test = metrics.mean_squared_error(y_test, y_test_pred, squared = False)\n",
    "    \n",
    "    print(\"Train RMSE = %.2f\" % rmse_train)\n",
    "    print(\"Test RMSE = %.2f\" % rmse_test)\n",
    "    \n",
    "    rsq_train = r_squared(y_train, y_train_pred, np.mean(y_train))\n",
    "    rsq_test = r_squared(y_test, y_test_pred, np.mean(y_train))\n",
    "    \n",
    "    print(\"Train R^2 = %.2f\" % rsq_train)\n",
    "    print(\"Test R^2 = %.2f\" % rsq_test)\n",
    "\n",
    "def train_eval_ols(X, y, split_seed = 1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state = split_seed)\n",
    "    \n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    eval_ols(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "#     return rsq_train, rsq_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: Why do we compare to `y_train` when calculating $R^2$ on the test set?\n",
    "\n",
    "\n",
    "Now we'll build and evaluate a series of OLS models together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Price ~ Accomodates\")\n",
    "train_eval_ols(listings[['accommodates']], listings['price'])\n",
    "print(\"\\nPrice ~ Accomodates + Reviews/Month\")\n",
    "train_eval_ols(listings[['accommodates','reviews_per_month']], listings['price'])\n",
    "print(\"\\nPrice ~ Accomodates + Reviews/Month + Rating\")\n",
    "train_eval_ols(listings[['accommodates','reviews_per_month','review_scores_rating']], listings['price'])\n",
    "print(\"\\nPrice ~ All Variables\")\n",
    "train_eval_ols(listings[features], listings['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question**: Can we say anything definitive about these results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In some situations, overfitting is more obvious -- and since it is a common problem when building models, we will now look at how it can be addressed.\n",
    "\n",
    "Regularization is a tool which helps us to avoid overfitting by penalising model complexity. Mathematically, we add a term to the loss function in the optimisation problem to be solved. Recall that the OLS formulation we've worked with is:\n",
    "\n",
    "$$\\min_w \\: \\frac{1}{n} \\sum_{i = 1}^{n} (w^\\intercal x_i - y_i)^2$$\n",
    "\n",
    "With a regularization term, this becomes:\n",
    "\n",
    "$$\\min_w \\: \\frac{1}{n} \\sum_{i = 1}^{n} (w^\\intercal x_i - y_i)^2 + \\lambda \\Omega(w)$$\n",
    "\n",
    "\n",
    "$\\Omega(w)$ is a penalty on the complexity of the model. Two common choices for $\\Omega(w)$ are:\n",
    "\n",
    "1. $\\Omega(w) = ||w||_2^2$: this is ridge regression.\n",
    "2. $\\Omega(w) = ||w||_1$: this is LASSO regression.\n",
    "\n",
    "Both types of regression shrink the elements of the optimal $w^*$ vector towards 0 -- but in different ways. We will focus on LASSO -- which tends to shrink the coefficients so that some are equal to 0. This is nice because it helps us interpret the model by making it *sparser*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "We need a way of selecting the one with the best parameter for our task.\n",
    "\n",
    "**Question**: How would we do this using the training/validation/testing splits?\n",
    "\n",
    "\n",
    "Here, because `GridSearchCV` makes it easy, we're going to use a similar technique called cross-validation. Generally, we only consider this necessary when we're worried we have too little training data to obtain an accurate estimate of validation performance.\n",
    "\n",
    "The idea behind cross-validation is: repeating the training/validation process multiple times provides us with several estimates of validation performance. Taking the average of these hopefully gives us an estimate which is less affected by noise.\n",
    "\n",
    "To cross-validate, we start with only two partitions of the dataset:\n",
    "\n",
    "1. Combined training/validation set: the data that we repeatedly train and validate on.\n",
    "2. Testing set: the data we obtain our final performance estimate on.\n",
    "\n",
    "But, how do we train and validate?\n",
    "\n",
    "- First, select a number of *folds*.\n",
    "- Then divide the training/validation data into this number of equal-sized partitions.\n",
    "- For each fold, repeat the training/validation procedure. The fold is the validation data, and the other folds are training data.\n",
    "- Average the performance of each model across folds and pick the hyperparameters which produce the best model.\n",
    "- Fit a model on the entire training set using the selected values of hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our training set from before and run 5-fold cross-validation to determine the best `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid that we want to search over\n",
    "grid = {'alpha':np.arange(0.01, 1, 0.01)}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(Lasso(max_iter=1e6), grid, cv=5)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m = gs.best_estimator_\n",
    "\n",
    "## Print the best parameters determined by the cross-validation\n",
    "print(\"Alpha: %.3f\" % gs.best_params_['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_output = pd.DataFrame({'feature':X_train.columns,\n",
    "              'coefficient':m.coef_})\n",
    "print(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\n",
    "coef_output.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our error changes with our penalty term (called alpha in the Lasso() implementation). As the penalty increases, the fitted models have more and more nonzero coefficients. We can visually see how thee error changes with lambda. It looks relatively flat at values near 0.4, meaning that we could potentially increase interpretability at close to no loss in prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_results = pd.DataFrame({'params':gs.cv_results_['params'], \n",
    "              'params_numeric':np.arange(0.01,1,0.01), \n",
    "              'score':gs.cv_results_['mean_test_score']})\n",
    "plt.plot(param_results['params_numeric'],param_results['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven't obtained an error measurement on the testing data, which will allow us to compare our best regularised model with the unregularised one. \n",
    "\n",
    "Let's generate predictions and calculate our performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_ols(gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well we do if we use a higher alpha parameter, even though its slightly suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Lasso(alpha=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "eval_ols(clf, X_train, y_train, X_test, y_test)\n",
    "print(\"Number of zeros: %d\" % sum(clf.coef_ == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous best model had a test RMSE of 82.86 (and $R^2$ of 0.41), so we now have a slightly better model. Regularization has also given us a way of obtaining an accurate model with fewer nonzero coefficients. Even when taking the higher suboptimal alpha, we still do nearly as well with even higher interpretability (5 fewer nonzeros than the optimal LASSO model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classification\n",
    "So far we've looked at models which predict a continuous response variable. There are many related models which predict categorical outcomes, such as whether an email is spam or not, or which digit a handwritten number is. We'll take a brief look at three of these: logistic regression and classification trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is part of the class of generalized linear models (GLMs), which build directly on top of linear regression. These models take the linear fit and map it through a non-linear function. Linear regression has the form $$y_i=w_0+w_1 x_{i,1}+w_2 x_{i,2}=w^Tx_i,$$ whereas a GLM model has the form $$y_i=f(w^Tx_i)$$ For logistic regression, the function $f()$ is given by $f(z) = 1/(1+\\exp(-z))$ (the *logistic function*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a logistic regression model to the `listings` data. Let's try to predict which listings have elevators in the building by using `price` as a predictor. Remember that our `amenity_Elevator_in_Building` variable already stores our binary outcome of interest (1 = elevator, 0 = no elevator).\n",
    "\n",
    "We need to re-split our data with y as the `amenity_Elevator_in_Building` variable. We will also *stratify* our split on this variable, meaning that the data is split to preserve the ratio of units with elevators in both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  listings.columns.drop(['id', 'scrape_id', 'host_id', 'latitude', 'longitude', \n",
    "                                   'host_total_listings_count','calculated_host_listings_count',\n",
    "                                   'availability_30','availability_60', 'availability_90', 'availability_365',\n",
    "                                   'amenity_Elevator_in_Building', 'ols_pred', 'ols_resid'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(listings[features], listings['amenity_Elevator_in_Building'], \n",
    "                                                    train_size = 0.7, random_state = 2,\n",
    "                                                   stratify = listings['amenity_Elevator_in_Building'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of linear regression, we'll now use `LogisticRegression()`, but the syntax is very similar. We'll skip ahead directly to using a grid search to tune regularization parameters. As you can see, the same steps apply even with a new model:\n",
    "1. Define a parameter grid\n",
    "2. Initialize a GridSearchCV object with the tuning grid and other fixed parameters\n",
    "3. Fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid that we want to search over\n",
    "param_grid = {'C': np.arange(0.001, 1, 0.05), \n",
    "              'penalty': ['l2','l1'], \n",
    "              'solver': ['liblinear']}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(LogisticRegression(random_state=42, max_iter = 1000),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m_lr = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the coefficients\n",
    "coef_output = pd.DataFrame({'feature':X_train.columns,\n",
    "              'coefficient':m_lr.coef_[0]})\n",
    "\n",
    "print(\"Number of zeros: %d\" % (coef_output.query('coefficient == 0').shape[0]))\n",
    "coef_output.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore out-of-sample performance. Ultimately, we want to predict whether or not a listing has an elevator. However, logistic regression gives us something a bit different: a probability that each listing has an elevator. This gives us flexibility in the way we predict. The most natural thing would be to predict that any listing with predicted probability above 0.5 *has* an elevator, and any listing with predicted probability below 0.5 *does not have* an elevator. But what if I use a wheelchair and I want to be really confident that there's going to be an elevator? I may want to use a cutoff value of 0.9 rather than 0.5. In fact, we could choose any cutoff value and have a corresponding prediction model.\t\n",
    "\n",
    "This is where AUC comes in. For every cutoff, we'll plot the *false positive rate* against the *true positive rate* and then take the area under this curve.\t\n",
    "\n",
    "**Question**: As a sanity check: What is the true positive rate and false positive rate of a random classifier that chooses \"has an elevator\" with probability of $\\alpha$? (i.e. a classifier that randomly predicts *positive* $\\alpha$% of the time.) What is the AUC for this classifier?\n",
    "\n",
    "scikit-learn allows us to easily plot ROC curves and calculate AUC. Here's an example. We'll write our evaluation in a function so that we can evaluate other models in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = m_lr.predict_proba(X_train)[:,1]\n",
    "test_pred = m_lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Train AUC: %.3f\" % metrics.roc_auc_score(y_train, train_pred))\n",
    "print(\"Test AUC: %.3f\" % metrics.roc_auc_score(y_test, test_pred))\n",
    "\n",
    "train_fpr, train_tpr, _ = metrics.roc_curve(y_train, train_pred)\n",
    "test_fpr, test_tpr, _ = metrics.roc_curve(y_test, test_pred)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "plt.plot(train_fpr, train_tpr, linestyle='--', label='Train')\n",
    "plt.plot(test_fpr, test_tpr, marker='.', label='Test')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at threshold-based metrics. By selecting a decision threshold $t$, we predict all observations with a predicted probability $\\hat{y} \\geq t$ to have an elevator and all others to have no elevator. \n",
    "\n",
    "Let's calculate the training and testing accuracy for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "print(\"Train Accuracy: %.3f\" %  metrics.accuracy_score(y_train, train_pred > t))\n",
    "print(\"Test Accuracy: %.3f\" %  metrics.accuracy_score(y_test, test_pred > t))\n",
    "\n",
    "# If we wanted to get all of the elements of the confusion matrix, we could pull them like this:\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_train, train_pred > t).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the baseline accuracy for this prediction task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, we'll wrap all of our evaluation steps into a function that we can use to evaluate other models in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluation(model, X_train, y_train, X_test, y_test, t = 0.5):\n",
    "    train_pred = model.predict_proba(X_train)[:,1]\n",
    "    test_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    print(\"Train AUC: %.3f\" % metrics.roc_auc_score(y_train, train_pred))\n",
    "    print(\"Test AUC: %.3f\" % metrics.roc_auc_score(y_test, test_pred))\n",
    "    \n",
    "    print(\"\\nTrain Accuracy: %.3f\" %  metrics.accuracy_score(y_train, train_pred > t))\n",
    "    print(\"Test Accuracy: %.3f\" %  metrics.accuracy_score(y_test, test_pred > t))\n",
    "\n",
    "    train_fpr, train_tpr, _ = metrics.roc_curve(y_train, train_pred)\n",
    "    test_fpr, test_tpr, _ = metrics.roc_curve(y_test, test_pred)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(train_fpr, train_tpr, linestyle='--', label='Train')\n",
    "    plt.plot(test_fpr, test_tpr, marker='.', label='Test')\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classification_evaluation(m_lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `sklearn.metrics` is versatile and allows you to calculate and plot a bunch of different performance metrics. In our case, this model gives a test AUC of 0.742. The worst possible is 0.5 - random guessing. We've built a pretty good model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees \n",
    "We will briefly explore classification trees (often referred to as CART, for Classification And Regression Trees).\n",
    "\n",
    "A (binary) classification tree makes predictions by grouping similar observations and then assigning a probability to each group using the proportion of observations within that group that belong to the positive class. Groups can be thought of as nodes on a tree, and tree branches correspond to logical criteria on the predictor variables. There's a lot of neat math that goes into building the trees, but we won't get into that today. For now let's get familiarized by looking at a simple example. We will use the `DecisionTreeClassifier` in scikit-learn.\t\n",
    "\n",
    "#### Tuning the CART model\n",
    "If we want to construct high accuracy decision tree models, then we need to tune the parameters.  CART has many parameters that specify how the decision tree is constructed. Today we'll focus on `max_depth`. \n",
    "* If the tree is too big => too many splits => we have an over-fitting problem.\n",
    "* If the tree is too small => too few splits => we have an under-fitting problem.\n",
    "\n",
    "We want to find a tree in the middle, that is \"just right\". We'll use `GridSearchCV` package to tune the tree depth using cross-validation. We'll also try out a couple of different criteria.\n",
    "\n",
    "The model construction step follows the same established pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the grid that we want to search over\n",
    "param_grid = {\"max_depth\": np.arange(3,10,1), \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m_cart = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot and save the resulting tree as follows:\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (50,10), dpi=200)\n",
    "plot_tree(m_cart, feature_names = X_train.columns, filled = False, fontsize = 8);\n",
    "fig.savefig('cart_tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our evaluation function from before to see how well this model performs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_evaluation(m_cart, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CART model does worse than the logistic regression model, with an AUC of 0.870 (vs. 0.9) on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "We will briefly take a look at random forests, using the [RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). A random forest is a collection of slightly randomized decision trees (hence the name \"forest\"), and can be used for classification or prediction. They often have excellent predictive performance, but can be expensive to train and lack interpretability. Random forests have many hyperparameters that can be tuned to achieve the best possible predictive performance. Perhaps the most important hyperparameter is the number of trees to include in the forest. More trees results in a longer training time but can improve prediction and decrease overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by training a random forest model for a classification task. We will perform the same task of predicting whether or not a listing has an elevator, using price and neighborhood as predictors. We will compare the performance of random forest to what we got using our simple CART model. We'll use a grid search to tune the number of estimators.\n",
    "\n",
    "*Note: This will take a few minutes!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,500,1000],\n",
    "     'max_depth': np.arange(5,10,1),\n",
    "     'max_features': ['auto'],\n",
    "     'min_samples_leaf': [0.01,0.02],\n",
    "    'criterion' :['gini']\n",
    "}\n",
    "        \n",
    "# Define the parameters for the model \n",
    "gs = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                  return_train_score=True, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='roc_auc',\n",
    "                  cv=5, verbose = 0)\n",
    "\n",
    "## Fit the model\n",
    "random.seed(1)\n",
    "gs.fit(X_train, y_train)\n",
    "m_rf = gs.best_estimator_\n",
    "print(\"Best parameters: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's evaluate the model performance\n",
    "classification_evaluation(m_rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to what we got using CART, which had training AUC of 0.87. The tuned random forest model improves on the CART model. In general, random forests give stronger performance than single decision trees, but there is a sacrifice in interpretability. Interestingly, the logistic regression model is slightly better than the random forest model. The prediction accuracy is now much better, and it doesn't look we are overfitting *too* much. It would likely be even better if the other hyperparameters of the random forest model were properly tuned! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting a Random Forest Model\n",
    "Although random forest models are not very interpretable and hard to visualize, there is a popular method called *variable importance* that is commonly used with random forest models. We will use the built-in `feature_importances_` in our random forest model. This tells us which variables the random forest model has determined are the most important for predicting whether or not a listing contains an elevator. It uses Gini importance, which is the mean decrease in node impurity (you can read more about this metric online), to rank the importance of all of the predictors used in the model. There are other metrics besides the Gini Importance that can also be used as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_output = pd.DataFrame({'feature':X_train.columns,\n",
    "                                 'importance':m_rf.feature_importances_}).sort_values('importance', ascending = False)\n",
    "\n",
    "plt.barh(importance_output['feature'][0:10], importance_output['importance'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable importance plot shows us that whether or not the unit has a gym, how many listings the host has, whether there is a doorman, and the unit price are the most important features. This not only gives us intuition about our random forest model, but can also be used to select variables to train other models. However, there are some shortcomings. For example, price is important in the model, but what is the direction of this relationship? Are high or low prices associated with elevator likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation with SHapley Additive exPlanations (SHAP)\n",
    "There has been a lot of research devoted to better methods of interpreting \"black box\" machine learning models, like random forests. The [SHAP](https://shap.readthedocs.io/en/latest/) package offers an easy-to-use interface that estimates the contribution of each variable to each observation's prediction. This gives insight into both the *magnitude* and *directionality* of a feature's importance. The package also allows for plots of interactions between variables to understand important nonlinearities. We'll do this for the random forest model, but we could use the same package for many other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  ## you can install this in terminal: 'conda install -c conda-forge shap'\n",
    "\n",
    "explainer = shap.TreeExplainer(m_rf); # data=test_x, model_output=\"probability\");\n",
    "shap_values = explainer.shap_values(X_train)[1]; # index pulls the P(y=1) SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary Plot\n",
    "We'll start with the summary plot, which ranks the features by importance (mean absolute SHAP value) and also shows the direction of the relationship. \n",
    "\n",
    "Features are ordered by decreasing significance, with the most important feature listed at the top of the plot. For a given feature, the corresponnding row shows a plot of the feature's impact on the prediction as the value ranges from its lowest (blue) to highest (red) value. Higher SHAP values correspond to increased likelihood of having a positive outcome (i.e. having an elevator). Thus, features with the color scale oriented blue to red (moving left to right) have increasing risk as the feature increases, such as hotel count. Note: the categorical variables are encoded as a binary value (e.g. 0=no gym, 1=gym), so \"higher\" values correspond to having the indicated feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "shap.summary_plot(shap_values, X_train, show=True,\n",
    "                  max_display=10,\n",
    "                  plot_type=\"violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary plot gives us insight into the key driving features; this is similar to Gini feature importance but gives us more granular insight.\n",
    "- Gym, Doorman, Washer, Wheelchair Accessible, Pool, and AC amenities all increase the probability of an elevator.\n",
    "- As price increeases, the probability of an elevator increases.\n",
    "- Listings from hosts with more listings have higher probability of an elevator.\n",
    "- The property type being a house decreases the probability.\n",
    "\n",
    "##### Dependence Plots\n",
    "\n",
    "Let's dive into the relationship between price and whether the unit is predicted to have an elevator. We can create a dependence plot or price to see how varied prices affect the output probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close()\n",
    "shap.dependence_plot('price', shap_values, X_train,\n",
    "                       interaction_index = None,\n",
    "                       xmin=\"percentile(1)\", xmax=\"percentile(99)\",\n",
    "                       dot_size=3,show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like listings for listings below \\\\$180/night, the price lowers thee probability that the unit will have an elevator (SHAP value < 0). Above \\\\$180/night, there is a positive impact on probability of having an elevator. Above \\\\$300/night or so, however, the impact on the probability stabilizes; the probability does not continue to increase with prices above \\\\$300. \n",
    "\n",
    "###### Force Plots\n",
    "Finally, force plots let us see how each feature contributes to the final probability for a given observation. The red features contribute to increases in the elevator probability, and the blue features decrease. The width of the bar (decreasing from the center) indicates the magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=1 # Try id=1 and id=5 to see a high and low probability example\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1],shap_values[id],\n",
    "               feature_names = X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "**Question:** Now that we have trained and evaluated these different models, which one would you select?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this module, we have covered examples of machine learning methods for linear regression (ordinary and penalized) and classification.  This is just the tip of the iceberg.  There are tons more machine learning methods which can be easily implemented in Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on sklearn modeling\n",
    "- While the examples above used CART and Random Forests for classification tasks, there is almost identical syntax to run them for regression tasks. We won't go through these in details, but check out [DecisionTreeRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) for more information. \n",
    "- There are many, many classification algorithms implemented in `scikit-learn`. [Here's](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) an example that compares a broad set of methods, including SVM, k-Nearest Neighbors, and others.\n",
    "- XGBoost follows nearly identical syntax to the examples above (using GridSearchCV). However, it is not implemented in scikit-learn. You can install the [xgboost](https://xgboost.readthedocs.io/en/latest/) library and then adapt the code above to this model."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
