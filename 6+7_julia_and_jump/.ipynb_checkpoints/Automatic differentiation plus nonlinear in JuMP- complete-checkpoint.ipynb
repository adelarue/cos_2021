{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation and nonliner optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on:\n",
    "- Arthur Delarue's notebook from COS 2018: https://github.com/PhilChodrow/cos_2018/blob/master/7_nonlinear_and_integer/Automatic%20Differentiation%20and%20Dual%20Numbers%20-%20Solutions.ipynb\n",
    "- material from https://nbviewer.jupyter.org/github/JuliaOpt/JuMPTutorials.jl/blob/master/notebook/using_JuMP/nonlinear_modelling.ipynb contributed to by Arpit Bhatia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing derivatives for nonlinear optimization using automatic differentiation\n",
    "Consider a general constrained nonlinear optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & f(x) \\\\\n",
    "\\text{s.t.} \\quad & g(x) = 0 \\\\\n",
    "& h(x) \\leq 0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f : \\mathbb{R}^n \\to \\mathbb{R}, g : \\mathbb{R}^n \\to \\mathbb{R}^r$, and $h: \\mathbb{R}^n \\to \\mathbb{R}^s$.\n",
    "\n",
    "When $f$ and $h$ are convex and $g$ is affine, we can hope for a globally optimal solution, otherwise typically we can only ask for a locally optimal solution.\n",
    "\n",
    "What approaches can we use to solve this?\n",
    "\n",
    "- When $r=0$ and $s = 0$ (unconstrained), and $f$ differentiable, most classical approach is [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent), also fancier methods like [Newton's method](http://en.wikipedia.org/wiki/Newton%27s_method) and quasi-newton methods like [BFGS](http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm).\n",
    "- When $f$ differentiable and $g$ and $h$ linear, [gradient projection](http://neos-guide.org/content/gradient-projection-methods),\n",
    "- When $f$, $g$, and $h$ twice differentiable, [interior-point methods](http://en.wikipedia.org/wiki/Interior_point_method), [sequential quadratic programming](http://www.neos-guide.org/content/sequential-quadratic-programming)\n",
    "- When derivatives not available, [derivative-free optimization](http://rd.springer.com/article/10.1007/s10898-012-9951-y)\n",
    "\n",
    "This is not meant to be an exhaustive list, see http://plato.asu.edu/sub/nlores.html#general and http://www.neos-guide.org/content/nonlinear-programming for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Numbers\n",
    "\n",
    "\n",
    "Consider numbers of the form $x + y\\epsilon$ with $x,y \\in \\mathbb{R}$. We *define* $\\epsilon^2 = 0$, so,\n",
    "$$\n",
    "(x_1 + y_1\\epsilon)(x_2+y_2\\epsilon) = x_1x_2 + (x_1y_2 + x_2y_1)\\epsilon.\n",
    "$$\n",
    "\n",
    "These are called the *dual numbers*. Think of $\\epsilon$ as an infinitesimal perturbation (you've probably seen hand-wavy algebra using $(dx)^2 = 0$ when computing integrals - this is the same idea).\n",
    "\n",
    "How can these funky *dual numbers* help us?\n",
    "If we are given an infinitely differentiable function in Taylor expanded form\n",
    "$$\n",
    "f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!} (x-a)^k\n",
    "$$\n",
    "it follows that \n",
    "$$\n",
    "f(x+y\\epsilon) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!} (x-a+y\\epsilon)^k = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!} (x-a)^k + y\\epsilon\\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!}\\binom{k}{1} (x-a)^{k-1} = f(x) + yf'(x)\\epsilon,\n",
    "$$\n",
    "\n",
    "Let's unpack what's going on here. We started with a function $f : \\mathbb{R} \\to \\mathbb{R}$. Dual numbers are *not* real numbers, so it doesn't even make sense to ask for the value $f(x+y\\epsilon)$ given $x+y\\epsilon \\in \\mathbb{D}$ (the set of dual numbers). But anyway we plugged the dual number into the Taylor expansion, and by using the algebra rule $\\epsilon^2 = 0$ we found that $f(x+y\\epsilon)$ must be equal to $f(x) + yf'(x)\\epsilon$ if we use the Taylor expansion as the definition of $f : \\mathbb{D} \\to \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, for any once differentiable function $f : \\mathbb{R} \\to \\mathbb{R}$, we can define its extension to the dual numbers as$$\n",
    "f(x+y\\epsilon) = f(x) + yf'(x)\\epsilon.\n",
    "$$This is essentially equivalent to the previous definition.\n",
    "\n",
    "Let's verify a very basic property, the chain rule, using this definition.\n",
    "\n",
    "Suppose $h(x) = f(g(x))$. Then,$$\n",
    "h(x+y\\epsilon) = f(g(x+y\\epsilon)) = f(g(x) + yg'(x)\\epsilon) = f(g(x)) + yg'(x)f'(g(x))\\epsilon = h(x) + yh'(x)\\epsilon.\n",
    "$$\n",
    "\n",
    "Maybe that's not too surprising, but it's actually a quite useful observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the package `ForwardDiff`, but there are others: e.g. `ReverseDiff`, `Zygote`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dual{Nothing}(3.0,4.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ForwardDiff.Dual(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ForwardDiff.Dual{Nothing,Float64,1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForwardDiff.value(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ForwardDiff.partials(d) # oops broken\n",
    "d.partials.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "+(x::<b>ForwardDiff.Dual{Txy,V,N} where N where V</b>, y::<b>ForwardDiff.Dual{Txy,V,N} where N where V</b>)<i> where Txy</i> in ForwardDiff at <a href=\"file://C:/Users/lkape/.julia/packages/ForwardDiff/DVizx/src/dual.jl\" target=\"_blank\">C:\\Users\\lkape\\.julia\\packages\\ForwardDiff\\DVizx\\src\\dual.jl:134</a>"
      ],
      "text/plain": [
       "+(x::ForwardDiff.Dual{Txy,V,N} where N where V, y::ForwardDiff.Dual{Txy,V,N} where N where V) where Txy in ForwardDiff at C:\\Users\\lkape\\.julia\\packages\\ForwardDiff\\DVizx\\src\\dual.jl:134"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which d + ForwardDiff.Dual(3.0,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect should be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+(z::Dual, w::Dual) = dual(real(z)+real(w), epsilon(z)+epsilon(w))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "*(x::<b>Dual{Txy,V,N} where N where V</b>, y::<b>Dual{Txy,V,N} where N where V</b>)<i> where Txy</i> in ForwardDiff at <a href=\"file://C:/Users/lkape/.julia/packages/ForwardDiff/DVizx/src/dual.jl\" target=\"_blank\">C:\\Users\\lkape\\.julia\\packages\\ForwardDiff\\DVizx\\src\\dual.jl:134</a>"
      ],
      "text/plain": [
       "*(x::Dual{Txy,V,N} where N where V, y::Dual{Txy,V,N} where N where V) where Txy in ForwardDiff at C:\\Users\\lkape\\.julia\\packages\\ForwardDiff\\DVizx\\src\\dual.jl:134"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which Dual(2.0,2.0) * Dual(3.0,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect should be:\n",
    "\n",
    "```*(z::Dual, w::Dual) = dual(real(z)*real(w), epsilon(z)*real(w)+real(z)*epsilon(w))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see it at work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic univariate functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dual{Nothing}(0.6931471805599453,0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(ForwardDiff.Dual(2.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeInfo(\n",
       "\u001b[90m1 ─\u001b[39m       nothing\n",
       "\u001b[90m│  \u001b[39m %2  = Base.getproperty(ForwardDiff, :value)\n",
       "\u001b[90m│  \u001b[39m       x = (%2)(d)\n",
       "\u001b[90m│  \u001b[39m       log#758 = ForwardDiff.log(x)\n",
       "\u001b[90m│  \u001b[39m       inv#759 = ForwardDiff.inv(x)\n",
       "\u001b[90m│  \u001b[39m       val = log#758\n",
       "\u001b[90m│  \u001b[39m       deriv = inv#759\n",
       "\u001b[90m│  \u001b[39m %8  = Base.getproperty(ForwardDiff, :Dual)\n",
       "\u001b[90m│  \u001b[39m %9  = Core.apply_type(%8, $(Expr(:static_parameter, 1)))\n",
       "\u001b[90m│  \u001b[39m %10 = val\n",
       "\u001b[90m│  \u001b[39m %11 = deriv\n",
       "\u001b[90m│  \u001b[39m %12 = Base.getproperty(ForwardDiff, :partials)\n",
       "\u001b[90m│  \u001b[39m %13 = (%12)(d)\n",
       "\u001b[90m│  \u001b[39m %14 = %11 * %13\n",
       "\u001b[90m│  \u001b[39m %15 = (%9)(%10, %14)\n",
       "\u001b[90m└──\u001b[39m       return %15\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@code_lowered log(ForwardDiff.Dual(2.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How general is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "squareroot (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squareroot(x)\n",
    "    z = x # Initial starting point\n",
    "    while abs(z * z - x) > 1e-13\n",
    "        z = z - (z * z - x) / (2z)\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squareroot(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we differentiate this code? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dual{Nothing}(10.0,0.049999999999999996)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = squareroot(ForwardDiff.Dual(100.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (2 * sqrt(100)) # The exact derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to do this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 0.05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squareroot(x)\n",
    "    z = x # Initial starting point\n",
    "    z_prime = 1\n",
    "    while abs(z * z - x) > 1e-13\n",
    "        z = z - (z * z - x) / (2z)\n",
    "        # apply the quotient rule:\n",
    "        z_prime = z_prime - ((2z * z_prime - 1) * 2z - (z * z - x) * 2z_prime) / (4 * z ^ 2)\n",
    "    end\n",
    "    return (z, z_prime)\n",
    "end\n",
    "squareroot(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not encouraging to start doing everything with nlp, but here is how it works. let's switch topics for a sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian likelihood is -\n",
    "\n",
    "$$\n",
    "L(\\theta | \\mathbf{x})=L\\left(\\theta_{1}, \\ldots, \\theta_{k} | x_{1}, \\ldots, x_{n}\\right)=\\prod_{i=1}^{n} f\\left(x_{i} | \\theta_{1}, \\ldots, \\theta_{k}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we want to maximize log-likelihood. \n",
    "We can use @NLobjective to do this and a nonlinear solver e.g. IPOPT to do this.\n",
    "\n",
    "IPOPT will use derivative information.\n",
    "We are allowed to plug-in log-likelihood directly into @NLobjective because it so happens that our log-likelihood calculation also uses functions with derivatives that are already defined in `Calculus.jl`, which JuMP will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is taken from https://nbviewer.jupyter.org/github/JuliaOpt/JuMPTutorials.jl/blob/master/notebook/using_JuMP/nonlinear_modelling.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use `@NLobjective` so that we minimize the log-likelihood.\n",
    "The likelihood is given by:\n",
    "$$\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-n/2} \\exp \\left( - \\sum_{i=1}^n (x_i - \\mu)^2 / (2 \\sigma^2) \\right)\n",
    "$$\n",
    "So the log-likelihood is:\n",
    "$$\n",
    "-\\frac{n}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maximize_log_likelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP\n",
    "using Ipopt\n",
    "\n",
    "function maximize_log_likelihood(x::Vector{Float64})\n",
    "    n = length(x)\n",
    "#     mle = Model(with_optimizer(Ipopt.Optimizer, print_level = 0)) # old syntax\n",
    "    mle = Model(Ipopt.Optimizer)\n",
    "    set_parameters(mle, \"print_level\" => 0)\n",
    "\n",
    "    @variable(mle, μ, start = 0.5)\n",
    "    @variable(mle, σ >= 0.0, start = 0.5)\n",
    "    \n",
    "    @NLobjective(mle, Max, log((2 * π * σ^2)^(-n / 2) * exp(-(sum((x[i] - μ)^2 for i in 1:n) / (2 * σ^2)))))\n",
    "    \n",
    "     optimize!(mle)\n",
    "    return (value(μ), value(σ))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.053307729269647004, 1.1062693505952153)\n",
      "(-0.09327136524012737, 0.9799742023885425)\n",
      "(-0.15231048673923142, 1.06951009379244)\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "using Random\n",
    "Random.seed!(1)\n",
    "println(maximize_log_likelihood(randn(10)))\n",
    "println(maximize_log_likelihood(randn(50)))\n",
    "println(maximize_log_likelihood(randn(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had a more involved function e.g. a function with a loop, output from a simulation, output from an ODE, or anything that is too tedious to work out derivatives for manually?\n",
    "\n",
    "We can register user-defined functions. JuMP will use AD to get derivative information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(1)\n",
    "x = randn(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loglikelihood (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loglikelihood(μ::Float64)\n",
    "    # assume we have σ = 1\n",
    "    n = length(x)\n",
    "    # make sure to work in logspace so that numbers don't \"explode\"\n",
    "    return -n / 2 * log(2 * π) - sum((x[i] - μ)^2 for i in 1:n) / 2\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: x not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: x not defined",
      "",
      "Stacktrace:",
      " [1] loglikelihood(::ForwardDiff.Dual{ForwardDiff.Tag{typeof(loglikelihood),Float64},Float64,1}) at .\\In[34]:4",
      " [2] derivative(::typeof(loglikelihood), ::Float64) at C:\\Users\\lkape\\.julia\\packages\\ForwardDiff\\DVizx\\src\\derivative.jl:14",
      " [3] top-level scope at In[35]:1"
     ]
    }
   ],
   "source": [
    "grad = ForwardDiff.derivative(loglikelihood, 0.78) # doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loglikelihood (generic function with 2 methods)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we typed our function too strictly, let's try again\n",
    "function loglikelihood(μ)\n",
    "    # assume we have σ = 1\n",
    "    n = length(x)\n",
    "    return -n / 2 * log(2 * π) - sum((x[i] - μ)^2 for i in 1:n) / 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.26692270730353"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = ForwardDiff.derivative(loglikelihood, 0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCALLY_SOLVED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05330772926964722"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mle = Model(with_optimizer(Ipopt.Optimizer, print_level = 0)) # old syntax\n",
    "mle = Model(Ipopt.Optimizer)\n",
    "set_parameters(mle, \"print_level\" => 0)\n",
    "register(mle, :loglikelihood, 1, loglikelihood, autodiff = true)\n",
    "@variable(mle, μ, start = 1.0)\n",
    "@NLobjective(mle, Max, loglikelihood(μ))\n",
    "optimize!(mle)\n",
    "println(termination_status(mle))\n",
    "value(μ)\n",
    "# previously mean was 0.053307729269647004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can obtain analytic derivatives for your function, you can also supply them instead of relying on `ForwardDiff.jl`, see the JuMP docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other packages  out there e.g. `Optim.jl` in https://github.com/JuliaNLSolvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [1.00e+00, 1.00e+00]\n",
       "    Minimum:   5.191703e-27\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     L-BFGS\n",
       "    Initial Point: [0.00e+00, 0.00e+00]\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = 4.58e-11 ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = 4.58e-11 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = 4.41e-19 ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = 8.50e+07 ≰ 0.0e+00\n",
       "    |g(x)|                 = 1.44e-13 ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    24\n",
       "    f(x) calls:    67\n",
       "    ∇f(x) calls:   67\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x0 = [0.0, 0.0]\n",
    "optimize(f, x0, LBFGS(); autodiff = :forward)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0-DEV",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
